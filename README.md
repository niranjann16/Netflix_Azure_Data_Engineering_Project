# Netflix_Azure_Data_Engineering_Project

This project is a data engineering solution built on Microsoft Azure, focused on processing Netflix data using the Medallion Architecture (Bronze, Silver, and Gold layers). It automates data workflows to provide scalable and efficient processing from raw data to analytics-ready insights.

## **System Architecture Overview**

The Medallion Architecture guides the data transformation, where the raw data flows through the following stages:

1. **Bronze Layer** - Raw, unrefined data
2. **Silver Layer** - Cleaned and enriched data
3. **Gold Layer** - Analytics-ready data


![Medallion architecture](https://github.com/niranjann16/Netflix_Azure_Data_Engineering_Project/blob/main/Project_screenshot/Architecture1.jpg)

![Architecture](https://github.com/niranjann16/Netflix_Azure_Data_Engineering_Project/blob/main/Project_screenshot/Architecture.png)

---

## **Project Overview**

This project implements a robust data pipeline for Netflix data using the Medallion framework. By leveraging Azure tools, the pipeline is designed to handle large-scale data processing and provide structured insights at the end.

### **Key Features**

- **Three-Tier Architecture:** Data is processed and stored in separate stages (Bronze, Silver, Gold) for easy management and optimization.
- **Automated Pipeline:** Azure Data Factory orchestrates the entire pipeline, moving and transforming data automatically.
- **Incremental Loading:** Data is added incrementally to the Bronze layer, ensuring efficient processing.
- **Data Cleanup:** Databricks handles data transformations, making the final dataset ready for analytics.

---

## **Technologies Used**

- **Azure Data Factory** for orchestrating data movement.
- **Azure Data Lake Gen 2** for tiered storage (Bronze, Silver, Gold).
- **Azure Databricks** for data transformation and enrichment.

---

## **Project Workflow**

### **Step 1: Raw Data Ingestion**
Data is fetched from an external source (e.g., GitHub) and loaded into Azure SQL Database using Azure Data Factory.

### **Step 2: Incremental Data Loading**
New data is loaded incrementally into the Bronze layer of Azure Data Lake Gen 2. Azure Data Factory automates this process.

### **Step 3: Data Transformation**
Databricks processes the data, cleans it, and applies necessary transformations to build the Silver and Gold layers, structured into a star schema.

### **Step 4: Reporting-Ready Data**
The Gold layer is the final outputâ€”cleaned and enriched data that can be used for business intelligence or reporting.

---

## **Pipeline Details**

### **Pipeline 1: Data Ingestion and Incremental Data Loading**

This pipeline fetches data from GitHub and loads it into the Bronze layer Azure Data in Lake Gen2.

![Incremental DataPipeline](https://github.com/niranjann16/Netflix_Azure_Data_Engineering_Project/blob/main/Project_screenshot/Incremental%20DataPipeline.png)

### **Databricks Workflow**

The raw data is transformed into structured tables in Databricks.

![databricks workflow](https://github.com/niranjann16/Netflix_Azure_Data_Engineering_Project/blob/main/Project_screenshot/Workflow.png)

### **Updated Incremental Data Loading Pipeline**

To enhance the efficiency of data transformation, a Databricks notebook has been integrated with the Databrick workflow pipeline. 

- The Databricks notebook is triggered at the end of the incremental loading process in ADB.
- Once the data is ingested into the Bronze layer of Azure Data Lake Gen 2, the Databricks notebook processes and moves it to the Silver and Gold layers.
- The notebook handles data cleansing, transformation, and structuring, making the final dataset ready for analytical use.

![updated_increm_pipeline](https://github.com/niranjann16/Netflix_Azure_Data_Engineering_Project/blob/main/Project_screenshot/Workflow1.png)

### **Azure Resource Group**

Overview of the deployed resources within Azure for this project.

![resource group](https://github.com/Bhumin-Patel029/CarsProject_Images/blob/main/Resource_Group.png)

---

## **How to Deploy and Run the Project**

Follow these steps to deploy and run the project in your Azure environment.

### **Prerequisites**
- **Azure Subscription** for access to Azure services (Data Factory, Databricks, Data Lake).
- **Basic knowledge of Azure Data Factory** for pipeline creation.
- **Azure Databricks Workspace** setup and cluster configuration.
- **GitHub Repository Access** for fetching the raw data.

### **Step-by-Step Instructions**

1. **Configure Data Factory Pipelines:**
   - Pipeline 1: Set up data ingestion from GitHub to Azure Data Lake Gen2.
   - Pipeline 2: Automate incremental data loading into the Bronze layer of Data Lake.

2. **Set Up Databricks:**
   - Create a Databricks workspace and set up the cluster.
   - Import Databricks notebooks that handle data transformation and schema creation.

3. **Verify Data Flow:**
   - Ensure raw data is stored in containers (DataLake).
   - Verify that data flows correctly through the Bronze, Silver, and Gold layers in Data Lake.

4. **Optional Customizations:**
   - Modify workflows and notebooks as needed, including adding custom monitoring or error handling mechanisms.

---

### **Post-Execution**

After the pipeline completes, the Gold layer will contain clean, enriched data. This data can be accessed and used for analytics or reporting.

---

